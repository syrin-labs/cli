# Syrin Configuration File
# ========================
# This file configures your Syrin runtime environment.
# For detailed documentation, visit: https://github.com/syrin-labs/cli

# Configuration Version
# The version of this configuration schema.
version: "{{VERSION}}"

# Project Configuration
# ---------------------
# Project name: A unique identifier for your MCP server project
project_name: "{{PROJECT_NAME}}"

# Agent name: The name of your AI agent that will interact with the MCP server
agent_name: "{{AGENT_NAME}}"

# Transport Configuration
# -----------------------
# Transport type: How Syrin communicates with your MCP server
#   - "stdio": Communicate via standard input/output (spawns the MCP server process)
#   - "http":  Communicate via HTTP (connects to a running MCP server)
transport: "{{TRANSPORT}}"

{{#IF_HTTP}}
# MCP Server URL (required for http transport)
# The HTTP endpoint where your MCP server is running
# Example: "http://localhost:8000/mcp"
url: "{{URL}}"
{{/IF_HTTP}}
{{#IF_STDIO}}
# MCP Server URL (not used for stdio transport)
# Uncomment and configure if you switch to http transport
# url: "http://localhost:8000/mcp"
{{/IF_STDIO}}

# Script Configuration
# --------------------
# Command to run your MCP server
# For stdio transport, this is required
# Use --run-script flag in dev mode to spawn the server internally
{{#IF_SCRIPT}}
script: "{{SCRIPT}}"
{{/IF_SCRIPT}}
{{#IF_NO_SCRIPT}}
# script: "python3 server.py"
{{/IF_NO_SCRIPT}}

# LLM Provider Configuration
# ---------------------------
# Configure one or more LLM providers for Syrin to use.
# Use environment variable names (keys in .env) for API_KEY and MODEL_NAME.
# At least one provider must be marked as default: true

llm:
{{#LLM_PROVIDERS}}
  # {{PROVIDER_NAME}} Provider
  {{PROVIDER_NAME}}:
{{#IF_OLLAMA}}
    # Model name: env var name in .env (e.g. OLLAMA_MODEL_NAME)
    MODEL_NAME: "{{MODEL_NAME}}"
{{/IF_OLLAMA}}
{{#IF_CLOUD_PROVIDER}}
    # API key: env var name in .env (e.g. {{PROVIDER_NAME_UPPER}}_API_KEY)
    API_KEY: "{{API_KEY}}"

    # Model name: env var name in .env (e.g. {{PROVIDER_NAME_UPPER}}_MODEL_NAME)
    MODEL_NAME: "{{MODEL_NAME}}"
{{/IF_CLOUD_PROVIDER}}
{{#IF_DEFAULT}}
    # Set as default LLM provider
    default: true
{{/IF_DEFAULT}}
{{#IF_NOT_DEFAULT}}
    default: false
{{/IF_NOT_DEFAULT}}
{{#IF_OPENAI_AND_NO_CLAUDE}}

  # Claude Provider (uncomment to enable)
  # claude:
  #   API_KEY: "CLAUDE_API_KEY"
  #   MODEL_NAME: "CLAUDE_MODEL_NAME"
  #   default: false
{{/IF_OPENAI_AND_NO_CLAUDE}}
{{#IF_CLAUDE_AND_NO_OPENAI}}

  # OpenAI Provider (uncomment to enable)
  # openai:
  #   API_KEY: "OPENAI_API_KEY"
  #   MODEL_NAME: "OPENAI_MODEL_NAME"
  #   default: false
{{/IF_CLAUDE_AND_NO_OPENAI}}
{{#IF_NO_OLLAMA}}

  # Ollama Provider - uncomment to enable
  # ollama:
  #   MODEL_NAME: "OLLAMA_MODEL_NAME"
  #   default: false
{{/IF_NO_OLLAMA}}

{{/LLM_PROVIDERS}}
# Tool Validation Configuration
# ---------------------------
# Configure tool-level structural safety validation
# This section is optional but recommended for production use

check:
  # Timeout for tool execution in milliseconds
  timeout_ms: 30000

  # Tools directory name (relative to project root, default: "tools")
  # tools_dir: tools

  # Maximum output size in KB (default: 50)
  # max_output_size_kb: 50

  # Strict mode: warnings become errors (default: false)
  # strict_mode: false
