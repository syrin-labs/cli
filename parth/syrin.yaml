# Syrin Configuration File
# ========================
# This file configures your Syrin runtime environment.
# For detailed documentation, visit: https://github.com/syrin-labs/cli

# Configuration Version
# The version of this configuration schema.
version: '1.0.0'

# Project Configuration
# ---------------------
# Project name: A unique identifier for your MCP server project
project_name: 'parth'

# Agent name: The name of your AI agent that will interact with the MCP server
agent_name: 'Parth'

# Transport Configuration
# -----------------------
# Transport type: How Syrin communicates with your MCP server
#   - "stdio": Communicate via standard input/output (spawns the MCP server process)
#   - "http":  Communicate via HTTP (connects to a running MCP server)
transport: 'http'

# MCP Server URL (required for http transport)
# The HTTP endpoint where your MCP server is running
# Example: "http://localhost:8000/mcp"
mcp_url: 'http://localhost:8000/mcp'

# Script Configuration
# --------------------
# Command to run your MCP server
# For stdio transport, this is required
# Use --run-script flag in dev mode to spawn the server internally
# For testing with error-seeded tools, use: script: 'python3 server.analyse.py'
script: 'python3 server.analyse.py'

# LLM Provider Configuration
# ---------------------------
# Configure one or more LLM providers for Syrin to use.
# API keys and model names can be set as:
#   - Environment variable names (recommended for security)
#   - Direct values (not recommended for production)
# At least one provider must be marked as default: true

llm:
  # OPENAI Provider
  openai:
    # API key: Set as environment variable name or direct value
    # Example: "OPENAI_API_KEY" (reads from process.env.OPENAI_API_KEY)
    # Or: "sk-..." (direct value, not recommended)
    API_KEY: 'OPENAI_API_KEY'

    # Model name: Set as environment variable name or direct value
    # Example: "OPENAI_MODEL_NAME" (reads from process.env.OPENAI_MODEL_NAME)
    # Or: "gpt-4" (direct value)
    MODEL_NAME: 'OPENAI_MODEL_NAME'

    # Set as default LLM provider
    default: true

  # Claude Provider (uncomment to enable)
  claude:
    API_KEY: 'CLAUDE_API_KEY'
    MODEL_NAME: 'CLAUDE_MODEL_NAME'
    default: false

  # Ollama Provider - uncomment to enable
  ollama:
    MODEL_NAME: 'OLLAMA_MODEL_NAME' # or "llama2", "mistral", etc.
    default: false

# Tool Validation Configuration (v1.3.0)
# ---------------------------
# Configure tool-level structural safety validation
check:
  # Timeout for tool execution in milliseconds
  timeout_ms: 30000

  # Tools directory name (relative to project root, default: "tools")
  tools_dir: tools

  # Maximum output size in KB (default: 50)
  max_output_size_kb: 50

  # Strict mode: warnings become errors (default: false)
  strict_mode: false
