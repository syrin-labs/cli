# Syrin Configuration File
# ========================
# This file configures your Syrin runtime environment.
# For detailed documentation, visit: https://github.com/AnkanAI/syrin

# Configuration Version
# The version of this configuration schema.
version: '1.0.0'

# Project Configuration
# ---------------------
# Project name: A unique identifier for your MCP server project
project_name: 'http-py'

# Agent name: The name of your AI agent that will interact with the MCP server
agent_name: 'Agent'

# Transport Configuration
# -----------------------
# Transport type: How Syrin communicates with your MCP server
#   - "stdio": Communicate via standard input/output (spawns the MCP server process)
#   - "http":  Communicate via HTTP (connects to a running MCP server)
transport: 'http'

# Command (required for stdio transport)
# The command to start your MCP server process
# Example: "python3 server.py" or "node server.js"
# command: "python3 server.py"

# MCP Server URL (not used for stdio transport)
# Uncomment and configure if you switch to http transport
mcp_url: 'http://localhost:8000/mcp'

# Script Configuration
# --------------------
# Scripts that Syrin can use to run your MCP server
#   - dev:   Command used during development (e.g., "python3 server.py --dev")
#   - start: Command used in production (e.g., "python3 server.py")
scripts:
  dev: 'python3 server.py'
  start: 'python3 server.py'

# LLM Provider Configuration
# ---------------------------
# Configure one or more LLM providers for Syrin to use.
# API keys and model names can be set as:
#   - Environment variable names (recommended for security)
#   - Direct values (not recommended for production)
# At least one provider must be marked as default: true

llm:
  # OPENAI Provider
  openai:
    # API key: Set as environment variable name or direct value
    # Example: "OPENAI_API_KEY" (reads from process.env.OPENAI_API_KEY)
    # Or: "sk-..." (direct value, not recommended)
    API_KEY: 'OPENAI_API_KEY'

    # Model name: Set as environment variable name or direct value
    # Example: "OPENAI_MODEL_NAME" (reads from process.env.OPENAI_MODEL_NAME)
    # Or: "gpt-4" (direct value)
    MODEL_NAME: 'OPENAI_MODEL_NAME'

    # Set as default LLM provider
    default: true

  # Claude Provider (uncomment to enable)
  # claude:
  #   API_KEY: "CLAUDE_API_KEY"
  #   MODEL_NAME: "CLAUDE_MODEL_NAME"
  #   default: false

  # Local LLM Provider (Ollama) - uncomment to enable
  # llama:
  #   provider: "ollama/local"
  #   command: "<command to run from local>"
  #   default: false
